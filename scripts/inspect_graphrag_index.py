"""
Inspect GraphRAG index files after indexing completes.

Reads and summarizes the parquet files generated by GraphRAG indexing:
- create_final_entities.parquet
- create_final_relationships.parquet
- create_final_communities.parquet
- create_final_community_reports.parquet
- create_final_text_units.parquet
"""

import os
import logging
from pathlib import Path
from typing import Optional, Dict, Any
import pandas as pd

logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")
logger = logging.getLogger(__name__)


def find_latest_output(ragtest_root: str = "./ragtest") -> Optional[Path]:
    """
    Find the latest output directory in ragtest/output/.

    Handles two GraphRAG output formats:
    1. New format: files directly in ragtest/output/ (e.g., entities.parquet)
    2. Old format: ragtest/output/<timestamp>/artifacts/ (e.g., create_final_entities.parquet)

    Returns:
        Path to output directory containing parquet files, or None if not found
    """
    output_dir = Path(ragtest_root) / "output"

    if not output_dir.exists():
        return None

    # Check if parquet files exist directly in output_dir (new GraphRAG format)
    parquet_files = list(output_dir.glob("*.parquet"))
    if parquet_files:
        return output_dir

    # Check for timestamped subdirectories (old GraphRAG format)
    subdirs = [d for d in output_dir.iterdir() if d.is_dir()]

    if not subdirs:
        return None

    # Return most recent (by name, which is timestamp)
    return max(subdirs, key=lambda d: d.name)


def inspect_entities(artifacts_dir: Path) -> Dict[str, Any]:
    """Inspect entities parquet file."""
    # Try both naming conventions
    entities_file = artifacts_dir / "entities.parquet"
    if not entities_file.exists():
        entities_file = artifacts_dir / "create_final_entities.parquet"

    if not entities_file.exists():
        return {"error": "File not found"}

    df = pd.read_parquet(entities_file)

    stats = {
        "count": len(df),
        "columns": list(df.columns),
        "file_size_mb": entities_file.stat().st_size / 1024 / 1024,
    }

    # Get top entities by degree if available
    if "degree" in df.columns:
        # Use 'title' column (new format) or 'name' column (old format)
        name_col = "title" if "title" in df.columns else "name"
        cols = [name_col, "type", "degree"] if "type" in df.columns else [name_col, "degree"]
        top_entities = df.nlargest(10, "degree")[cols].to_dict("records")
        stats["top_entities"] = top_entities
        stats["name_column"] = name_col

    return stats


def inspect_relationships(artifacts_dir: Path) -> Dict[str, Any]:
    """Inspect relationships parquet file."""
    # Try both naming conventions
    relationships_file = artifacts_dir / "relationships.parquet"
    if not relationships_file.exists():
        relationships_file = artifacts_dir / "create_final_relationships.parquet"

    if not relationships_file.exists():
        return {"error": "File not found"}

    df = pd.read_parquet(relationships_file)

    stats = {
        "count": len(df),
        "columns": list(df.columns),
        "file_size_mb": relationships_file.stat().st_size / 1024 / 1024,
    }

    # Get relationship type distribution if available
    if "type" in df.columns:
        type_counts = df["type"].value_counts().head(10).to_dict()
        stats["top_types"] = type_counts

    # Get weight statistics if available (new format)
    if "weight" in df.columns:
        stats["weight_stats"] = {
            "mean": float(df["weight"].mean()),
            "median": float(df["weight"].median()),
            "max": float(df["weight"].max()),
            "min": float(df["weight"].min()),
        }

    return stats


def inspect_communities(artifacts_dir: Path) -> Dict[str, Any]:
    """Inspect communities parquet file."""
    # Try both naming conventions
    communities_file = artifacts_dir / "communities.parquet"
    if not communities_file.exists():
        communities_file = artifacts_dir / "create_final_communities.parquet"

    if not communities_file.exists():
        return {"error": "File not found"}

    df = pd.read_parquet(communities_file)

    stats = {
        "count": len(df),
        "columns": list(df.columns),
        "file_size_mb": communities_file.stat().st_size / 1024 / 1024,
    }

    # Get community size distribution if available
    if "size" in df.columns:
        stats["size_stats"] = {
            "mean": float(df["size"].mean()),
            "median": float(df["size"].median()),
            "max": int(df["size"].max()),
            "min": int(df["size"].min()),
        }

    return stats


def inspect_community_reports(artifacts_dir: Path) -> Dict[str, Any]:
    """Inspect community reports parquet file."""
    # Try both naming conventions
    reports_file = artifacts_dir / "community_reports.parquet"
    if not reports_file.exists():
        reports_file = artifacts_dir / "create_final_community_reports.parquet"

    if not reports_file.exists():
        return {"error": "File not found"}

    df = pd.read_parquet(reports_file)

    stats = {
        "count": len(df),
        "columns": list(df.columns),
        "file_size_mb": reports_file.stat().st_size / 1024 / 1024,
    }

    # Get report length statistics if available
    if "full_content" in df.columns:
        df["content_length"] = df["full_content"].str.len()
        stats["content_stats"] = {
            "mean_length": float(df["content_length"].mean()),
            "median_length": float(df["content_length"].median()),
            "max_length": int(df["content_length"].max()),
        }

    return stats


def inspect_text_units(artifacts_dir: Path) -> Dict[str, Any]:
    """Inspect text units parquet file."""
    # Try both naming conventions
    text_units_file = artifacts_dir / "text_units.parquet"
    if not text_units_file.exists():
        text_units_file = artifacts_dir / "create_final_text_units.parquet"

    if not text_units_file.exists():
        return {"error": "File not found"}

    df = pd.read_parquet(text_units_file)

    stats = {
        "count": len(df),
        "columns": list(df.columns),
        "file_size_mb": text_units_file.stat().st_size / 1024 / 1024,
    }

    return stats


def print_stats(stats_dict: Dict[str, Dict[str, Any]]) -> None:
    """Pretty print statistics."""
    print("")
    print("=" * 80)
    print("GraphRAG Index Inspection Report")
    print("=" * 80)
    print("")

    # Entities
    print("ENTITIES")
    print("-" * 80)
    entities = stats_dict.get("entities", {})
    if "error" in entities:
        print(f"  Error: {entities['error']}")
    else:
        print(f"  Total entities: {entities['count']:,}")
        print(f"  File size: {entities['file_size_mb']:.2f} MB")
        if "top_entities" in entities:
            print(f"  Top 10 entities by degree:")
            name_col = entities.get("name_column", "name")
            for i, entity in enumerate(entities["top_entities"], 1):
                name = entity.get(name_col, entity.get("name", entity.get("title", "N/A")))
                print(f"    {i}. {name} (type: {entity.get('type', 'N/A')}, degree: {entity['degree']})")
    print("")

    # Relationships
    print("RELATIONSHIPS")
    print("-" * 80)
    relationships = stats_dict.get("relationships", {})
    if "error" in relationships:
        print(f"  Error: {relationships['error']}")
    else:
        print(f"  Total relationships: {relationships['count']:,}")
        print(f"  File size: {relationships['file_size_mb']:.2f} MB")
        if "top_types" in relationships:
            print(f"  Top relationship types:")
            for rel_type, count in list(relationships["top_types"].items())[:5]:
                print(f"    {rel_type}: {count:,}")
        if "weight_stats" in relationships:
            weight = relationships["weight_stats"]
            print(f"  Weight statistics:")
            print(f"    Mean: {weight['mean']:.2f}")
            print(f"    Median: {weight['median']:.2f}")
            print(f"    Range: {weight['min']:.2f} - {weight['max']:.2f}")
    print("")

    # Communities
    print("COMMUNITIES")
    print("-" * 80)
    communities = stats_dict.get("communities", {})
    if "error" in communities:
        print(f"  Error: {communities['error']}")
    else:
        print(f"  Total communities: {communities['count']:,}")
        print(f"  File size: {communities['file_size_mb']:.2f} MB")
        if "size_stats" in communities:
            size = communities["size_stats"]
            print(f"  Size statistics:")
            print(f"    Mean: {size['mean']:.1f}")
            print(f"    Median: {size['median']:.1f}")
            print(f"    Range: {size['min']} - {size['max']}")
    print("")

    # Community Reports
    print("COMMUNITY REPORTS")
    print("-" * 80)
    reports = stats_dict.get("community_reports", {})
    if "error" in reports:
        print(f"  Error: {reports['error']}")
    else:
        print(f"  Total reports: {reports['count']:,}")
        print(f"  File size: {reports['file_size_mb']:.2f} MB")
        if "content_stats" in reports:
            content = reports["content_stats"]
            print(f"  Content length statistics:")
            print(f"    Mean: {content['mean_length']:.0f} chars")
            print(f"    Median: {content['median_length']:.0f} chars")
            print(f"    Max: {content['max_length']:,} chars")
    print("")

    # Text Units
    print("TEXT UNITS")
    print("-" * 80)
    text_units = stats_dict.get("text_units", {})
    if "error" in text_units:
        print(f"  Error: {text_units['error']}")
    else:
        print(f"  Total text units: {text_units['count']:,}")
        print(f"  File size: {text_units['file_size_mb']:.2f} MB")
    print("")

    # Total size
    total_size = sum(
        s.get("file_size_mb", 0)
        for s in stats_dict.values()
        if "error" not in s
    )
    print(f"Total index size: {total_size:.2f} MB")
    print("=" * 80)
    print("")


def main():
    """Main entry point."""
    logger.info("Inspecting GraphRAG index files...")

    try:
        # Find latest output directory
        latest_output = find_latest_output()

        if latest_output is None:
            logger.error("No GraphRAG output directory found in ./ragtest/output/")
            logger.error("Run 'make graphrag-index' first to create the index.")
            return

        # Determine where parquet files are located
        # New format: directly in output dir
        # Old format: in output/<timestamp>/artifacts/
        if list(latest_output.glob("*.parquet")):
            # New format - files directly in output dir
            artifacts_dir = latest_output
        else:
            # Old format - files in artifacts subdirectory
            artifacts_dir = latest_output / "artifacts"
            if not artifacts_dir.exists():
                logger.error(f"Artifacts directory not found: {artifacts_dir}")
                return

        logger.info(f"Inspecting index from: {artifacts_dir}")

        # Inspect all parquet files
        stats = {
            "entities": inspect_entities(artifacts_dir),
            "relationships": inspect_relationships(artifacts_dir),
            "communities": inspect_communities(artifacts_dir),
            "community_reports": inspect_community_reports(artifacts_dir),
            "text_units": inspect_text_units(artifacts_dir),
        }

        # Print report
        print_stats(stats)

    except Exception as e:
        logger.error(f"Failed to inspect index: {e}")
        raise


if __name__ == "__main__":
    main()
